# Optimization_ML

This project is concerned with going beyond first-order techniques in machine learning. Algorithmic
frameworks such as gradient descent and stochastic gradient are inherently first-order methods, in
that they rely solely on first-order derivatives. Second-order methods, on the other hand, make
use of higher-order information, either explicitly or implicitly. Although those techniques are widely
used in scientific computing, their use in machine learning has yet to be generalized. The goal of
this project is to illustrate the performance of these techniques on learning problems involving both
synthetic and real data.
